{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa256f2",
   "metadata": {},
   "source": [
    "# 0. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea49f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 13:54:36.090607: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-03 13:54:38.339898: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-03 13:54:38.340085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.342694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.56GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-08-03 13:54:38.342721: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-03 13:54:38.345418: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-08-03 13:54:38.345492: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-08-03 13:54:38.346612: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-08-03 13:54:38.346880: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-08-03 13:54:38.350054: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-08-03 13:54:38.350718: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-08-03 13:54:38.350834: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-08-03 13:54:38.350945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.353562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.356051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-08-03 13:54:38.356409: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-03 13:54:38.356744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.359206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.56GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-08-03 13:54:38.359286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.361777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:38.364227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-08-03 13:54:38.364263: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-03 13:54:39.046545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-08-03 13:54:39.046579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-08-03 13:54:39.046586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-08-03 13:54:39.046781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:39.048519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:39.050041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 13:54:39.051523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13606 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a93aeb",
   "metadata": {},
   "source": [
    "# 1. 학습시킬 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790c5c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>새상품입니다\\n찐 아님</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s급 se2 화이트 64g 팝니다\\n사자마자 범퍼끼우고 다녀서 기스하나 없습니다\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>용량  64\\n \\n개통일  18813\\n액정 깨끗하고 테두리 찍힘만 살짝있는\\n기...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배터리 88퍼 \\n수리</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n갤럭시노트10 아우라글로우색상 256G 판매합니다\\n외관상태 아래사진과같이 매우...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23581</th>\n",
       "      <td>그을려서 있구요\\n사용하는데 문제는 없지만\\n고데기에 찍힘 메인보드이상으로\\n와이파...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23582</th>\n",
       "      <td>갤럭시 노트10 5g 256기가 팝니다 \\n유심 3사가능하구요 그냥 유심 끼고 바로...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23583</th>\n",
       "      <td>잔상 딸애주려고 찍힘없고 충전단자옆 기스만 있어요 기기빠르고 버벅거림없고 있으며 배...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23584</th>\n",
       "      <td>외관은 폰으로 구입해서 처분합니다\\n뒷판에 바코드 붙어있음 용량확인\\n기능 및 더큰...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23585</th>\n",
       "      <td>구글 입니다 128기가 입니다 \\n측면 약간의 휨 있어 가만하여 올려요 \\n안전거래...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23586 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    desc  label\n",
       "0                                          새상품입니다\\n찐 아님       0\n",
       "1      s급 se2 화이트 64g 팝니다\\n사자마자 범퍼끼우고 다녀서 기스하나 없습니다\\n...      1\n",
       "2      용량  64\\n \\n개통일  18813\\n액정 깨끗하고 테두리 찍힘만 살짝있는\\n기...      3\n",
       "3                                          배터리 88퍼 \\n수리       2\n",
       "4      \\n갤럭시노트10 아우라글로우색상 256G 판매합니다\\n외관상태 아래사진과같이 매우...      1\n",
       "...                                                  ...    ...\n",
       "23581  그을려서 있구요\\n사용하는데 문제는 없지만\\n고데기에 찍힘 메인보드이상으로\\n와이파...      4\n",
       "23582  갤럭시 노트10 5g 256기가 팝니다 \\n유심 3사가능하구요 그냥 유심 끼고 바로...      4\n",
       "23583  잔상 딸애주려고 찍힘없고 충전단자옆 기스만 있어요 기기빠르고 버벅거림없고 있으며 배...      4\n",
       "23584  외관은 폰으로 구입해서 처분합니다\\n뒷판에 바코드 붙어있음 용량확인\\n기능 및 더큰...      4\n",
       "23585  구글 입니다 128기가 입니다 \\n측면 약간의 휨 있어 가만하여 올려요 \\n안전거래...      4\n",
       "\n",
       "[23586 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('최종학습데이터_전처리완_EDA완.csv')\n",
    "data.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "data.columns = ['desc', 'label']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cf3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b88d80",
   "metadata": {},
   "source": [
    "# 2. 학습, 검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffface72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = data['desc'].to_list()\n",
    "train_labels = data['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f98d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5e041",
   "metadata": {},
   "source": [
    "# 3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63a8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "\n",
    "# Tokenizing\n",
    "train_encodings = tokenizer(train_texts, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, return_tensors='pt', truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ca4bd",
   "metadata": {},
   "source": [
    "# 4. 학습을 위한 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af020f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa785c1a",
   "metadata": {},
   "source": [
    "# 5. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daeb75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 13:55:06.774481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-08-03 13:55:07.279805: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LMKor_model = TFBertForSequenceClassification.from_pretrained('kykim/bert-kor-base', num_labels=6, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "LMKor_model.compile(optimizer=optimizer, loss=LMKor_model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca89915",
   "metadata": {},
   "source": [
    "# 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91fbfcfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "2022-08-03 13:55:19.107499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-08-03 13:55:19.217169: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2499990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359/2359 [==============================] - 580s 239ms/step - loss: 0.6865 - accuracy: 0.7674 - val_loss: 0.5299 - val_accuracy: 0.8336\n",
      "Epoch 2/15\n",
      "2359/2359 [==============================] - 568s 241ms/step - loss: 0.4181 - accuracy: 0.8663 - val_loss: 0.4336 - val_accuracy: 0.8671\n",
      "Epoch 3/15\n",
      "2359/2359 [==============================] - 550s 233ms/step - loss: 0.3020 - accuracy: 0.9071 - val_loss: 0.4418 - val_accuracy: 0.8796\n",
      "Epoch 4/15\n",
      "2359/2359 [==============================] - 541s 229ms/step - loss: 0.2448 - accuracy: 0.9267 - val_loss: 0.3950 - val_accuracy: 0.8875\n",
      "Epoch 5/15\n",
      "2359/2359 [==============================] - 544s 231ms/step - loss: 0.2010 - accuracy: 0.9439 - val_loss: 0.3836 - val_accuracy: 0.8951\n",
      "Epoch 6/15\n",
      "2359/2359 [==============================] - 549s 233ms/step - loss: 0.1793 - accuracy: 0.9473 - val_loss: 0.4054 - val_accuracy: 0.9021\n",
      "Epoch 7/15\n",
      "2359/2359 [==============================] - 557s 236ms/step - loss: 0.1505 - accuracy: 0.9557 - val_loss: 0.4238 - val_accuracy: 0.8964\n",
      "Epoch 8/15\n",
      "2359/2359 [==============================] - 569s 241ms/step - loss: 0.1389 - accuracy: 0.9608 - val_loss: 0.3369 - val_accuracy: 0.9178\n",
      "Epoch 9/15\n",
      "2359/2359 [==============================] - 558s 236ms/step - loss: 0.1212 - accuracy: 0.9658 - val_loss: 0.3913 - val_accuracy: 0.9167\n",
      "Epoch 10/15\n",
      "2359/2359 [==============================] - 556s 236ms/step - loss: 0.1263 - accuracy: 0.9655 - val_loss: 0.4552 - val_accuracy: 0.8843\n",
      "Epoch 11/15\n",
      "2359/2359 [==============================] - 555s 235ms/step - loss: 0.1087 - accuracy: 0.9698 - val_loss: 0.4300 - val_accuracy: 0.9144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff326bde880>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "    patience=3)\n",
    "\n",
    "callback_modelcheckpoint = ModelCheckpoint(\n",
    "    filepath='tf_model.h5',\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "LMKor_model.fit(\n",
    "    train_dataset.shuffle(1000).batch(8), epochs=15, batch_size=8,\n",
    "    validation_data=val_dataset.shuffle(1000).batch(8),\n",
    "    callbacks = [callback_earlystop, callback_modelcheckpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f422cc",
   "metadata": {},
   "source": [
    "# 7. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bfcf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_model/fine-tuned-kykim-bert-base -- Folder create complete \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('end_model/fine-tuned-kykim-bert-base/tokenizer_config.json',\n",
       " 'end_model/fine-tuned-kykim-bert-base/special_tokens_map.json',\n",
       " 'end_model/fine-tuned-kykim-bert-base/vocab.txt',\n",
       " 'end_model/fine-tuned-kykim-bert-base/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODEL_NAME = 'fine-tuned-kykim-bert-base'\n",
    "MODEL_SAVE_PATH = os.path.join(\"end_model\", MODEL_NAME) # change this to your preferred location\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder already exists \\n\")\n",
    "else:\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder create complete \\n\")\n",
    "\n",
    "# save tokenizer, model\n",
    "LMKor_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87cb7e",
   "metadata": {},
   "source": [
    "# 8. 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54aedea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at end_model/fine-tuned-kykim-bert-base were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at end_model/fine-tuned-kykim-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# Load Fine-tuning model\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH, id2label={0: 0 , 1: 1, 2: 2, 3: 3, 4: 4, 5: 5})\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87171b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_excel('테스트셋.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc4ffdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "predicted_score_list = []\n",
    "\n",
    "for text in testset['desc']:\n",
    "    # predict\n",
    "    preds_list = text_classifier(text)\n",
    "\n",
    "    sorted_preds_list = sorted(preds_list, key=lambda x: x['score'], reverse=True)\n",
    "    predicted_label_list.append(sorted_preds_list[0]['label']) # label\n",
    "    predicted_score_list.append(sorted_preds_list[1]['score']) # score\n",
    "testset['pred'] = predicted_label_list\n",
    "testset['score'] = predicted_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93dabc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        11\n",
      "           1       1.00      0.69      0.82        13\n",
      "           2       0.73      0.65      0.69        17\n",
      "           3       0.70      1.00      0.82        14\n",
      "           4       0.81      0.81      0.81        16\n",
      "           5       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.84      0.83      0.83        89\n",
      "weighted avg       0.83      0.82      0.82        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=testset['label'], y_pred=testset['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75627efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
